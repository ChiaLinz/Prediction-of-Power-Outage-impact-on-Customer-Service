{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to create dataset with outages and weather information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paths to folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = 'temp_2'\n",
    "unclean_data_folder = 'historical_data_raw'\n",
    "weather_data = 'weather_data'\n",
    "\n",
    "Path(output_folder).mkdir(exist_ok=True)\n",
    "Path(unclean_data_folder).mkdir(exist_ok=True)\n",
    "Path(weather_data).mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates clean csv files with relevant information for each annual summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_area(df):\n",
    "    df_states_only = df[~df['Area'].str.contains('County|Parish', na=False)]\n",
    "    df_county_only = df[df['Area'].str.contains('County|Parish', na=False)]\n",
    "\n",
    "    states_split = df_states_only['Area'].str.split(r': |, ',regex=True)\n",
    "    counties_split = df_county_only['Area'].str.split(r'; |: ', regex=True) # Special case for mulitiple states in a single row\n",
    "\n",
    "    # Appends county count to end of state string (ex Alabama:2)\n",
    "    def count_counties(x):\n",
    "        num_counties = len(x[1].split(', '))\n",
    "        x[0] = x[0] + f':{num_counties}'\n",
    "        if ', ' in x[0]:\n",
    "            x[0] = x[0].split(', ')[-1]\n",
    "        return [x[0]]\n",
    "\n",
    "    counties_split = counties_split.apply(count_counties)\n",
    "    df['Area'] = pd.concat([states_split, counties_split])\n",
    "\n",
    "    # Expands each element of the list into its own row\n",
    "    df = df.explode('Area', ignore_index=True)\n",
    "\n",
    "    # Cleans string\n",
    "    def custom_clean(x):\n",
    "        if x[-1] in [';', ':']:\n",
    "            x = x[:-1]\n",
    "        if x[-1] == ']':\n",
    "            x = x[:x.index('[')]\n",
    "        return x\n",
    "    df['Area'] = df['Area'].apply(custom_clean)\n",
    "    df = df.rename(columns={\"Area\": \"State\"})\n",
    "    df['State'] = df['State'].apply(lambda x: x + ':-1' if ':' not in x else x)\n",
    "    df['Num Counties Affected'] = df['State'].apply(lambda x: x.split(':')[1])\n",
    "    df['State'] = df['State'].apply(lambda x: x.split(':')[0])\n",
    "\n",
    "    df['Num Counties Affected'] = df['Num Counties Affected'].apply(pd.to_numeric)\n",
    "    df['Num Counties Affected'] = df['Num Counties Affected'].apply(lambda x: np.nan if x == -1 else x)\n",
    "\n",
    "    return df\n",
    "\n",
    "def clean_data(df):\n",
    "    '''Cleans annual summary files to save relevant data'''\n",
    "\n",
    "    # Isolate outages caused by weather and remove unusable/unecessary data\n",
    "    df = df[df['Event Type'].str.contains('Weather', na=False)]\n",
    "    df = df[~df['Date of Restoration'].astype(str).str.contains('Unknown', na=False)]\n",
    "    df.drop(columns=['Month', 'Alert Criteria', 'Event Type', 'Demand Loss (MW)', 'NERC Region'], inplace=True, errors='ignore')\n",
    "    df.dropna(axis='columns', how='all', inplace=True)\n",
    "\n",
    "    # Convert dates and times to consistent format\n",
    "    df['Date Event Began'] = pd.to_datetime(df['Date Event Began'])\n",
    "    df['Date Event Began'] = df['Date Event Began'].dt.date\n",
    "    df['Date of Restoration'] = pd.to_datetime(df['Date of Restoration'])\n",
    "    df['Date of Restoration'] = df['Date of Restoration'].dt.date\n",
    "    df['Time Event Began'] = pd.to_datetime(df['Time Event Began'].astype(str))\n",
    "    df['Time Event Began'] = df['Time Event Began'].dt.time\n",
    "    df['Time of Restoration'] = pd.to_datetime(df['Time of Restoration'].astype(str))\n",
    "    df['Time of Restoration'] = df['Time of Restoration'].dt.time\n",
    "\n",
    "    # Rename cols for clarity\n",
    "    df = df.rename(columns={\"Date Event Began\": \"Start Date\", \"Time Event Began\": \"Start Time\", \n",
    "        \"Date of Restoration\": \"End Date\", \"Time of Restoration\": \"End Time\", \"Area Affected\": \"Area\"})\n",
    "\n",
    "    # Create dest filepath if it doesn't exist and save to csv\n",
    "    df = split_area(df)\n",
    "    df['Number of Customers Affected'] = pd.to_numeric(df['Number of Customers Affected'], errors='coerce')\n",
    "\n",
    "    return df\n",
    "\n",
    "files = Path(f'{unclean_data_folder}/').glob('*_Annual_Summary.xls')\n",
    "for file in sorted(files):\n",
    "    name = file.name[:-4]\n",
    "    df = pd.read_excel(f'{unclean_data_folder}/{name}.xls', header=1)\n",
    "    df = clean_data(df)\n",
    "    df.to_csv(f'{output_folder}/{name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to remove time cols from annual summary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = Path(f'{output_folder}/').glob('*_Annual_Summary.csv')\n",
    "for file in sorted(files):\n",
    "    name = file.name\n",
    "    df = pd.read_csv(f'{output_folder}/{name}')\n",
    "    df.drop(columns=['Start Time', 'End Time'], inplace=True, errors='ignore')\n",
    "    df['Date'] = [pd.date_range(s, e, freq='d') for s, e in\n",
    "              zip(pd.to_datetime(df['Start Date']),\n",
    "                  pd.to_datetime(df['End Date']))]\n",
    "    df = df.explode('Date', ignore_index=True).drop(['Start Date', 'End Date'], axis=1)\n",
    "    df = df[['Date', 'State', 'Number of Customers Affected', 'Num Counties Affected']]\n",
    "    df.to_csv(Path(f'{output_folder}/{name}'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to create merged dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = Path(f'{output_folder}/').glob('*_Annual_Summary.csv')\n",
    "lst = sorted([str(path) for path in data_list])\n",
    "df = pd.concat(map(pd.read_csv, lst), ignore_index=True)\n",
    "df.to_csv(Path(f'{output_folder}/' + 'merged_data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to create seperate csvs for each state\n",
    "\n",
    "Block 1 - CSVs for states in dataframe\n",
    "Block 2 - CSVs for states not in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{output_folder}/merged_data.csv')\n",
    "for state in df.State.unique():\n",
    "    state_format = state.replace(' ', '_')\n",
    "    state_df = df.loc[df['State'] == state]\n",
    "    file = f'{output_folder}/states/{state_format}.csv'\n",
    "    path = Path(file)\n",
    "    path.parents[0].mkdir(parents=True, exist_ok=True)\n",
    "    state_df.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "unkown_states = ['Alaska', 'Hawaii', 'Montana', 'New_Mexico', 'Utah', 'Wyoming']\n",
    "for state_name in unkown_states:\n",
    "    state_df = pd.DataFrame(columns=['Date','State','Number of Customers Affected','Num Counties Affected'])\n",
    "    file = f'{output_folder}/states/{state_name}.csv'\n",
    "    state_df.to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to expand dates, label outage days per state, and add season feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = dt.datetime(year=2017, month=1, day=1).date()\n",
    "end_date = dt.datetime(year=2021, month=6, day=21).date()\n",
    "seasons = ['Winter', 'Winter', 'Spring', 'Spring', 'Spring', 'Summer', 'Summer', 'Summer', 'Fall', 'Fall', 'Fall', 'Winter']\n",
    "month_to_season = dict(zip(range(1,13), seasons))\n",
    "\n",
    "\n",
    "files = Path(f'{output_folder}/states/').glob('*.csv')\n",
    "for file in files:\n",
    "    name = f'{output_folder}/states/{file.name}'\n",
    "    state = file.name[0:-4]\n",
    "    state = state.replace('_', ' ')\n",
    "    df = pd.read_csv(f'{name}')\n",
    "    df['Outage'] = 1\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "    df = df.set_index('Date')\n",
    "\n",
    "    if start_date not in df.index:\n",
    "        row = {'State': state, 'Outage': 0}\n",
    "        temp = pd.DataFrame(data=row, index=[pd.to_datetime(start_date)])\n",
    "        df = pd.concat([df, temp])\n",
    "    if end_date not in df.index:\n",
    "        row = {'State': state, 'Outage': 0}\n",
    "        temp = pd.DataFrame(data=row, index=[pd.to_datetime(end_date)])\n",
    "        df = pd.concat([df, temp])\n",
    "\n",
    "    df = df.sort_index() \n",
    "\n",
    "    df = df.resample('D').agg({'Outage': np.sum, 'Num Counties Affected': np.max, 'Number of Customers Affected': np.max})   \n",
    "    df['State'] = state\n",
    "    df = df[['State', 'Outage', 'Number of Customers Affected', 'Num Counties Affected']]\n",
    "    df.index.name = 'Date'\n",
    "\n",
    "    df['Outage'][df['Outage'] >= 1] = 1\n",
    "    df['Number of Customers Affected'] = df.apply(lambda x: 0 if x['Outage'] == 0 else x['Number of Customers Affected'], axis=1)\n",
    "    df['Num Counties Affected'] = df.apply(lambda x: 0 if x['Outage'] == 0 else x['Num Counties Affected'], axis=1)\n",
    "    df['Season'] = df.index.month.map(month_to_season)\n",
    "\n",
    "    df.to_csv(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to clean the unclean weather data csvs and append to state file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Path(f'{weather_data}/')\n",
    "subdirectories = [x for x in p.iterdir() if x.is_dir()]\n",
    "for folder in subdirectories:\n",
    "    state = folder.name\n",
    "    file = Path(f'{weather_data}/{state}/').glob('*_weather_new.csv')\n",
    "    path = next(file)\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    df.drop(columns=['STATION'], inplace=True, errors='ignore')\n",
    "\n",
    "    df = df.set_index('DATE')\n",
    "    df = df.sort_index() \n",
    "    df = df.resample('D').mean()\n",
    "    df.index.name = 'Date'\n",
    "    df = df.rename(columns={'WT01': 'Fog', 'WT03': 'Thunder', 'WT05': 'Hail', 'WT07': 'Dust', 'WT10': 'Tornado', 'WT11': \n",
    "        'Wind', 'WT16': 'Rain', 'WT18': 'Snow'})\n",
    "    df = df.apply(lambda x: x.fillna(0) if x.name in ['Fog', 'Thunder', 'Hail', 'Dust', 'Tornado', 'Wind', 'Rain', 'Snow'] else x)\n",
    "    df.to_csv(f'{weather_data}/{state}/{path.name[:-4]}_clean.csv')\n",
    "\n",
    "    state_path = f'{output_folder}/states/{state}.csv'\n",
    "    state_df = pd.read_csv(f'{state_path}')\n",
    "    state_df['Date'] = pd.to_datetime(state_df['Date'])\n",
    "    fin = state_df.set_index('Date').join(df)\n",
    "    fin.to_csv(Path(state_path), index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to merge state weather files into a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = Path(f'{output_folder}/states').glob('*.csv')\n",
    "lst = sorted([str(path) for path in data_list])\n",
    "df = pd.concat(map(pd.read_csv, lst), ignore_index=True)\n",
    "df.to_csv(Path(f'{output_folder}/' + 'outage_n_weather_data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Links to manually download weather data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to get bounding boxes for each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "response = requests.get('https://gist.githubusercontent.com/a8dx/2340f9527af64f8ef8439366de981168/raw/81d876daea10eab5c2675811c39bcd18a79a9212/US_State_Bounding_Boxes.csv')\n",
    "r = response.text\n",
    "\n",
    "df = pd.DataFrame([x.split(',') for x in r.split('\\n')])\n",
    "df = df.rename(columns=df.iloc[0])\n",
    "df.drop([0, 57, 3, 8, 11, 14, 43, 49], inplace=True) # Remove non US states\n",
    "df.drop(['\"\"'], axis=1, inplace=True)\n",
    "df.columns = df.columns.str.replace('\"','')\n",
    "df['NAME'] = df.NAME.str.replace('\"','')\n",
    "df = df[[\"NAME\", \"ymax\", \"xmin\", \"ymin\", \"xmax\"]]\n",
    "df[['ymax', 'xmin', 'ymin', 'xmax']] = df[['ymax', 'xmin', 'ymin', 'xmax']].apply(pd.to_numeric)\n",
    "df = df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to get weather download url for each state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For datatype information, go to \n",
    "https://www.ncei.noaa.gov/data/global-historical-climatology-network-daily/doc/GHCND_documentation.pdf\n",
    "'''\n",
    "\n",
    "url = 'https://www.ncei.noaa.gov/access/search/data-search/daily-summaries'\n",
    "params = {\n",
    "    'dataTypes': ['AWND', 'PRCP', 'SNOW', 'SNWD', 'TMIN', 'TMAX', 'WT01', 'WT03', 'WT05', 'WT07', 'WT10', 'WT11', 'WT16', 'WT18'], \n",
    "    'startDate' : '2017-01-01T00:00:00', \n",
    "    'endDate': '2021-06-21T23:59:59', \n",
    "    'bbox': ''\n",
    "}\n",
    "\n",
    "\n",
    "with open('weather_urls.txt', 'w') as f:\n",
    "    i = 1\n",
    "    for index, row in df.iterrows():\n",
    "        ymax = row['ymax']\n",
    "        xmin = row['xmin']\n",
    "        ymin = row['ymin']\n",
    "        xmax = row['xmax']\n",
    "        bbox = f\"{ymax:.3f},{xmin:.3f},{ymin:.3f},{xmax:.3f}\"\n",
    "        params['bbox'] = bbox\n",
    "        #print(bbox)\n",
    "        r = requests.Request('GET', url, params=params)\n",
    "        prep = r.prepare()\n",
    "        f.write(row['NAME'] + '\\n' + prep.url + '\\n')\n",
    "        if i % 10 == 0:\n",
    "            f.write('\\n')\n",
    "        i += 1"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "beed513aa000dc89cfe3516f828ed62a753fed305c4c577c72f3959a1f4f8905"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 ('weatherForecastBankOfAmerica-bVgOoaF4')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
